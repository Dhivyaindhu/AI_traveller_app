# -*- coding: utf-8 -*-
"""Untitled143.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vsyUGX1MQjloFU7iw97HBFJq-PnpuF-v
"""

import streamlit as st
from langchain_core.prompts import ChatPromptTemplate
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.output_parsers import StrOutputParser
import os

# Set up Streamlit app title
st.title("AI Travel Cost Estimator")

# Use environment variable for security (Set this in your deployment)
gemini_API_KEY = os.getenv("GEMINI_API_KEY", "YOUR_DEFAULT_API_KEY")  # Replace "YOUR_DEFAULT_API_KEY" with a placeholder

# Check if API key is available
if gemini_API_KEY == " AIzaSyCBxouxQ_5gN9ikG_RK9a6WNUotzglMVmE":
    st.error("Please set your Gemini API key as an environment variable.")
else:
    # Define the prompt template
    chat_template = ChatPromptTemplate.from_messages([
        ("system", "You are an AI assistant who estimates travel costs for different modes of transport from {source} to {destination}. You provide cost breakdowns and recommendations for major places."),
        ("human", "Book a flight, train, bus, or car travel from {source} to {destination}.")
    ])

    # Initialize the chat model
    chat_model = ChatGoogleGenerativeAI(model="gemini-1.5-flash", google_api_key=gemini_API_KEY)

    # Define the processing pipeline
    parser = StrOutputParser()

    # Streamlit UI Inputs
    source = st.text_input("Enter Source Location:")
    destination = st.text_input("Enter Destination Location:")

    if st.button("Estimate Cost"):
        if source and destination:
            # Invoke the model with user input
            response = chat_model.invoke(chat_template.format_messages(source=source, destination=destination))

            # Parse and display the response
            parsed_response = parser.parse(response)
            st.write(parsed_response)
        else:
            st.warning("Please enter both source and destination.")